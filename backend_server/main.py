from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import List  # âœ… Python 3.8 ì§€ì›ì„ ìœ„í•œ typing import
import numpy as np
import torch
from backend_server.model import CNNBiLSTMModel

app = FastAPI()

# âœ… ì •í™•í•œ ë°ì´í„° íƒ€ì… ì§€ì • (Python 3.8 í˜¸í™˜)
class EEGInput(BaseModel):
    data: List[List[float]]  # Python 3.8 ì´ìƒì—ì„œ í˜¸í™˜

# ëª¨ë¸ ê²½ë¡œ ë° ì¥ì¹˜ ì„¤ì •
MODEL_PATH = "backend_server/saved_models/model_fold1_best.pt"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNBiLSTMModel(input_channels=8, input_time=38).to(DEVICE)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.eval()

@app.get("/")
def root():
    return {"message": "EEG seizure prediction model ready."}

@app.post("/predict")
async def predict(input_data: EEGInput, request: Request):
    try:
        arr = np.array(input_data.data)
        print("ğŸ”¥ ë“¤ì–´ì˜¨ EEG ë°ì´í„° shape:", arr.shape)

        if arr.shape != (8, 38):
            raise ValueError(f"âŒ ì…ë ¥ shape ì˜¤ë¥˜: {arr.shape} (ê¸°ëŒ€ê°’: (8, 38))")

        with torch.no_grad():
            tensor_input = torch.tensor(arr, dtype=torch.float32).unsqueeze(0).to(DEVICE)
            output = model(tensor_input).squeeze().item()
            prob = torch.sigmoid(torch.tensor(output)).item()

            print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ - í™•ë¥ : {prob:.4f}")
            return {
                "prediction": int(prob > 0.5),
                "probability": prob
            }

    except Exception as e:
        body = await request.body()
        print("âŒ ì˜ˆì™¸ ë°œìƒ:", e)
        print("ğŸ“¦ ìš”ì²­ ë³¸ë¬¸:", body.decode("utf-8"))
        raise HTTPException(status_code=400, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
